{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_questions = json.load(open('autocast_questions.json', encoding='utf-8')) # from the Autocast dataset\n",
    "test_questions = json.load(open('autocast_competition_test_set.json', encoding='utf-8'))\n",
    "test_ids = [q['id'] for q in test_questions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_questions = []\n",
    "tf = 0\n",
    "mc = 0\n",
    "nm = 0\n",
    "for question in autocast_questions:\n",
    "    if question[\"qtype\"] == \"t/f\" and question[\"answer\"] is not None and tf < 400:\n",
    "        subset_questions.append(question)\n",
    "        tf += 1\n",
    "    if question[\"qtype\"] == \"mc\" and question[\"answer\"] is not None and mc < 400:\n",
    "        subset_questions.append(question)\n",
    "        mc += 1\n",
    "    if question[\"qtype\"] == \"num\" and question[\"answer\"] is not None and nm < 400:\n",
    "        subset_questions.append(question)\n",
    "        nm += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6532\n",
      "0.05330348025635245\n"
     ]
    }
   ],
   "source": [
    "all_score_num = 0\n",
    "for question in autocast_questions:\n",
    "    if question[\"qtype\"] == \"num\" and question[\"answer\"]:\n",
    "        all_score_num += float(question[\"answer\"])\n",
    "print(len(autocast_questions))\n",
    "print(all_score_num / len(autocast_questions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline models outputting random answers\n",
    "# (This is useless now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrated_random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        pred_idx = np.argmax(np.random.random(size=2))\n",
    "        pred = np.ones(2)\n",
    "        pred[pred_idx] += 1e-2\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'mc':\n",
    "        pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "        pred = np.ones(len(question['choices']))\n",
    "        pred[pred_idx] += 1e-4\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return 0.415"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\"\n",
    "epsilone = 0.01\n",
    "def gpt35_model(question):\n",
    "    # model = \"text-curie-001\" # T/F: 43.02, MCQ: 70.47, NUM: 26.99 // not good at all \n",
    "    # model = \"davinci\" # T/F: 27.53, MCQ: 37.99, NUM: 22.63 // basically random\n",
    "    # model = \"text-davinci-002\" # T/F: 26.92，MCQ: 58.45，NUM: 34.24 // answer 95% all questions in correct form, but not often correct\n",
    "    model = \"text-davinci-003\" # // answer 99% questions in correct form, but not often correct\n",
    "    tf_question = \"You can only answer yes or no, \"\n",
    "    mc_question = \"You can only answer one singular character as the choice, \"\n",
    "    nm_question = \"You can only answer with one singular confidence number between 0 and 1, \"\n",
    "    tags = \"This question is related to \" + \" and \".join(question[\"tags\"]) + \". \"\n",
    "    background = \"The background is \" + question[\"background\"]\n",
    "    alphabets = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    if question['qtype'] == 't/f':\n",
    "        # response = openai.Completion.create(model=model, prompt=tf_question+tags+question[\"question\"], temperature=2, max_tokens=3)\n",
    "        # ans = response.choices[0].text[2:]\n",
    "        # if \"no\" in ans.lower():\n",
    "        #     return np.array([0.5+epsilone/10, 0.5-epsilone/10])\n",
    "        # elif \"yes\" in ans.lower():\n",
    "        #     return np.array([0.5-epsilone/10, 0.5+epsilone/10])\n",
    "        # else:\n",
    "        return np.array([0.5, 0.5])\n",
    "    elif question['qtype'] == 'mc':\n",
    "        pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "        preds = np.ones(len(question['choices']))\n",
    "        try:\n",
    "            choices = []\n",
    "            i = 0\n",
    "            while len(choices) != len(question[\"choices\"]):\n",
    "                choices.append(alphabets[i] + \": \" + list(question[\"choices\"])[i])\n",
    "                i+=1\n",
    "            response = openai.Completion.create(model=model, prompt=mc_question+tags+question[\"question\"]+\" Your choices are \"+str(choices), temperature=0, max_tokens=3, best_of=1)\n",
    "            ans = response.choices[0].text[2:]\n",
    "            if ans.upper() in alphabets and alphabets.index(ans.upper()) < len(choices):\n",
    "                preds[alphabets.index(ans.upper())] += 2.5\n",
    "                return preds / preds.sum()\n",
    "            else:\n",
    "                preds[pred_idx] += 1e-4\n",
    "                return preds / preds.sum()\n",
    "        except:\n",
    "            preds[pred_idx] += 0.06\n",
    "            return preds / preds.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return 0.415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get performance on the Autocast train set\n",
    "\n",
    "Note that the Autocast dataset contains questions in the competition test set. Those should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.375\n",
      "0.3856\n",
      "0.029999999999999992\n",
      "7.500000000000009e-07\n"
     ]
    }
   ],
   "source": [
    "def brier_score(probabilities, answer_probabilities):\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2\n",
    "\n",
    "print(brier_score(np.array([1, 0, 0, 0]), np.array([0, 0, 1, 0])))\n",
    "print(brier_score(np.array([0.25, 0.25, 0.25, 0.25]), np.array([0, 0, 1, 0])))\n",
    "print(brier_score(np.array([0.24, 0.28, 0.24, 0.24]), np.array([0, 0, 1, 0])))\n",
    "print(brier_score(np.array([0, 0.1, 0.8, 0.1]), np.array([0, 0, 1, 0])))\n",
    "print(brier_score(np.array([0, 0.0005, 0.999, 0.0005]), np.array([0, 0, 1, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.500000000000004\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "cutoff_date = datetime.datetime(2021, 9, 30)\n",
    "correct = 0\n",
    "for question in subset_questions:\n",
    "    if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if question['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    if datetime.datetime.fromisoformat(question[\"close_time\"][:-6]) > cutoff_date:\n",
    "        continue\n",
    "    preds.append(calibrated_random_baseline_model(question))\n",
    "    if question['qtype'] == 't/f':\n",
    "        ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('t/f')\n",
    "    elif question['qtype'] == 'mc':\n",
    "        ans_idx = ord(question['answer']) - ord('A')\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        if np.argmax(preds[-1]) == ans_idx:\n",
    "            correct += 1\n",
    "        qtypes.append('mc')\n",
    "    elif question['qtype'] == 'num':\n",
    "        ans = float(question['answer'])\n",
    "        qtypes.append('num')\n",
    "    answers.append(ans)\n",
    "print(correct / qtypes.count(\"mc\") * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: 25.01, MCQ: 37.59, NUM: 21.91\n",
      "Combined Metric: 84.51\n"
     ]
    }
   ],
   "source": [
    "tf_results, mc_results, num_results = [],[],[]\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for question in test_questions:\n",
    "    preds.append(calibrated_random_baseline_model(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'zip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('submission'):\n",
    "    os.makedirs('submission')\n",
    "\n",
    "with open(os.path.join('submission', 'predictions.pkl'), 'wb') as f:\n",
    "    pickle.dump(preds, f, protocol=2)\n",
    "\n",
    "!cd submission && zip ../submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90e47ca3de7a6ac5652c507781a9a883127089d6067d2cae315ebae4b66e7ceb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
