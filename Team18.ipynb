{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocast_questions = json.load(open('autocast_questions.json', encoding='utf-8')) # from the Autocast dataset\n",
    "test_questions = json.load(open('autocast_competition_test_set.json', encoding='utf-8'))\n",
    "test_ids = [q['id'] for q in test_questions]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create baseline models outputting random answers\n",
    "# (This is useless now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        return np.random.random(size=2)\n",
    "    elif question['qtype'] == 'mc':\n",
    "        probs = np.random.random(size=len(question['choices']))\n",
    "        return probs / probs.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return np.random.random()\n",
    "\n",
    "\n",
    "def calibrated_random_baseline_model(question):\n",
    "    if question['qtype'] == 't/f':\n",
    "        pred_idx = np.argmax(np.random.random(size=2))\n",
    "        pred = np.ones(2)\n",
    "        pred[pred_idx] += 1e-5\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'mc':\n",
    "        pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "        pred = np.ones(len(question['choices']))\n",
    "        pred[pred_idx] += 1e-5\n",
    "        return pred / pred.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        return 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "def gpt35_model(question):\n",
    "    model = \"text-curie-001\" # T/F: 43.02, MCQ: 70.47, NUM: 26.99\n",
    "    # model = \"davinci\" # T/F: 27.53, MCQ: 37.99, NUM: 22.63\n",
    "    # model = \"text-davinci-002\" # T/F: 26.92，MCQ: 58.45，NUM: 34.24\n",
    "    # model = \"text-davinci-003\" # Error\n",
    "    tf_question = \"You can only answer yes or no, \"\n",
    "    mc_question = \"You can only answer one singular character as the choice, \"\n",
    "    nm_question = \"You can only answer with one singular confidence number between 0 and 1. \"\n",
    "    alphabets = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "    if question['qtype'] == 't/f':\n",
    "        response = openai.Completion.create(model=model, prompt=tf_question+question[\"question\"], temperature=0, max_tokens=3)\n",
    "        ans = response.choices[0].text[2:]\n",
    "        if \"no\" in ans.lower():\n",
    "            return np.array([1, 0])\n",
    "        elif \"yes\" in ans.lower():\n",
    "            return np.array([0, 1])\n",
    "        else:\n",
    "            return np.array([0.5, 0.5])\n",
    "    elif question['qtype'] == 'mc':\n",
    "        try:\n",
    "            choices = []\n",
    "            i = 0\n",
    "            while len(choices) != len(question[\"choices\"]):\n",
    "                choices.append(alphabets[i] + \": \" + list(question[\"choices\"])[i])\n",
    "                i+=1\n",
    "            response = openai.Completion.create(model=model, prompt=mc_question+question[\"question\"]+\" Your choices are \"+str(choices), temperature=0, max_tokens=3)\n",
    "            ans = response.choices[0].text[2:]\n",
    "            preds = np.zeros(len(choices))\n",
    "            if ans.upper() in alphabets and alphabets.index(ans.upper()) < len(choices):\n",
    "                preds[alphabets.index(ans.upper())] = 1\n",
    "                return preds\n",
    "            else:\n",
    "                pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "                pred = np.ones(len(question['choices']))\n",
    "                pred[pred_idx] += 1e-5\n",
    "                return pred / pred.sum()\n",
    "        except:\n",
    "            pred_idx = np.argmax(np.random.random(size=len(question['choices'])))\n",
    "            pred = np.ones(len(question['choices']))\n",
    "            pred[pred_idx] += 1e-5\n",
    "            return pred / pred.sum()\n",
    "    elif question['qtype'] == 'num':\n",
    "        response = openai.Completion.create(model=model, prompt=question[\"question\"]+nm_question, temperature=0, max_tokens=30)\n",
    "        try:\n",
    "            ans = response.choices[0].text[2:]\n",
    "            if float(ans) < 0:\n",
    "                return 0\n",
    "            elif 0 <= float(ans) <= 1:\n",
    "                return float(ans)\n",
    "            elif float(ans) > 1:\n",
    "                return 1\n",
    "        except:\n",
    "            return 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get performance on the Autocast train set\n",
    "\n",
    "Note that the Autocast dataset contains questions in the competition test set. Those should not be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.375\n",
      "0.029999999999999992\n"
     ]
    }
   ],
   "source": [
    "def brier_score(probabilities, answer_probabilities):\n",
    "    return ((probabilities - answer_probabilities) ** 2).sum() / 2\n",
    "\n",
    "print(brier_score(np.array([0.25, 0.25, 0.25, 0.25]), np.array([0, 0, 1, 0])))\n",
    "print(brier_score(np.array([0, 0.1, 0.8, 0.1]), np.array([0, 0, 1, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "answers = []\n",
    "qtypes = []\n",
    "for question in autocast_questions:\n",
    "    if question['id'] in test_ids: # skipping questions in the competition test set\n",
    "        continue\n",
    "    if question['answer'] is None: # skipping questions without answer\n",
    "        continue\n",
    "    preds.append(gpt35_model(question))\n",
    "    if len(preds) % 100 == 0:\n",
    "        print(len(preds))\n",
    "    if question['qtype'] == 't/f':\n",
    "        ans_idx = 0 if question['answer'] == 'no' else 1\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('t/f')\n",
    "    elif question['qtype'] == 'mc':\n",
    "        ans_idx = ord(question['answer']) - ord('A')\n",
    "        ans = np.zeros(len(question['choices']))\n",
    "        ans[ans_idx] = 1\n",
    "        qtypes.append('mc')\n",
    "    elif question['qtype'] == 'num':\n",
    "        ans = float(question['answer'])\n",
    "        qtypes.append('num')\n",
    "    answers.append(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T/F: 43.02, MCQ: 70.47, NUM: 26.99\n",
      "Combined Metric: 140.48\n"
     ]
    }
   ],
   "source": [
    "tf_results, mc_results, num_results = [],[],[]\n",
    "for p, a, qtype in zip(preds, answers, qtypes):\n",
    "    if qtype == 't/f':\n",
    "        tf_results.append(brier_score(p, a))\n",
    "    elif qtype == 'mc':\n",
    "        mc_results.append(brier_score(p, a))\n",
    "    else:\n",
    "        num_results.append(np.abs(p - a))\n",
    "\n",
    "print(f\"T/F: {np.mean(tf_results)*100:.2f}, MCQ: {np.mean(mc_results)*100:.2f}, NUM: {np.mean(num_results)*100:.2f}\")\n",
    "print(f\"Combined Metric: {(np.mean(tf_results) + np.mean(mc_results) + np.mean(num_results))*100:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "nc = 0\n",
    "for question in test_questions:\n",
    "    try:\n",
    "        if len(list(question[\"choices\"])) > 26:\n",
    "            nc +=1\n",
    "    except:\n",
    "        continue\n",
    "print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for question in test_questions:\n",
    "    preds.append(gpt35_model(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'zip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('submission'):\n",
    "    os.makedirs('submission')\n",
    "\n",
    "with open(os.path.join('submission', 'predictions.pkl'), 'wb') as f:\n",
    "    pickle.dump(preds, f, protocol=2)\n",
    "\n",
    "!cd submission && zip ../submission.zip ./* && cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autocast_competition_test_set.json \u001b[36msubmission\u001b[m\u001b[m\n",
      "autocast_questions.json            submission.zip\n",
      "example_submission.ipynb\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90e47ca3de7a6ac5652c507781a9a883127089d6067d2cae315ebae4b66e7ceb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
